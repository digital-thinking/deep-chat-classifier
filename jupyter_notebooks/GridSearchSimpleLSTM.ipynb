{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv as csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import log\n",
    "%matplotlib inline\n",
    "#read cvs data with panda\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_json('messages.json', encoding='utf8', lines=True)\n",
    "print ('..done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import keras.backend as backend\n",
    "import mmh3 as mmh3\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "myDf = df[df['language'].isin(['en','de','it','fr','nl'])]\n",
    "\n",
    "def replaceUmlauts(string):\n",
    "    return unidecode.unidecode(string)    \n",
    "\n",
    "def preprocess(doc):\n",
    "    if doc is None:\n",
    "        return None     \n",
    "    \n",
    "    doc = doc.lower()\n",
    "    doc = replaceUmlauts(doc)\n",
    "  \n",
    "    words = text_to_word_sequence(doc, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~')    \n",
    "    \n",
    "    cleaned = []    \n",
    "    for x in words:\n",
    "        if (len(x) <= 2 or len(x) > 20):\n",
    "            continue\n",
    "        if (x.isdigit()):         \n",
    "            cleaned.append('token_number')  \n",
    "            continue\n",
    "        if (any(char.isdigit() for char in x)):\n",
    "            cleaned.append('token_hashlike')  \n",
    "            continue\n",
    "\n",
    "        y = re.sub('\\W+','', x)       \n",
    "        cleaned.append(y)        \n",
    "    return cleaned\n",
    "        \n",
    "        \n",
    "print(\"Feature cleaning\")\n",
    "myDf.message = myDf.message.map(preprocess)\n",
    "print ('..done')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building dict\")\n",
    "wordset = dict()\n",
    "counts = dict()\n",
    "index = 0\n",
    "\n",
    "def addToDict(wordseq):\n",
    "    for w in wordseq:\n",
    "        global index\n",
    "        if (w not in counts):\n",
    "            counts[w] = 1\n",
    "        else:\n",
    "            counts[w] += 1\n",
    "            if (w not in wordset):\n",
    "                wordset[w] = index\n",
    "                index += 1\n",
    "       \n",
    "    \n",
    "\n",
    "myDf['message'].apply(addToDict)\n",
    "\n",
    "\n",
    "print \"Uniqe words \" + str(len(counts))\n",
    "print \">= 2 occurancies \" + str(len(wordset))\n",
    "print ('..done')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transform to vector\")\n",
    "\n",
    "def getOrNone(word):\n",
    "    if (word in wordset):\n",
    "        return wordset[word]\n",
    "    return None    \n",
    "    \n",
    "\n",
    "def extract(sentence):    \n",
    "    items = map(getOrNone, sentence)    \n",
    "    return [x for x in items if x is not None]\n",
    "    \n",
    "\n",
    "myDf['nlp_features'] = myDf.message.map(extract)\n",
    "print ('..done')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = len(wordset)\n",
    "\n",
    "def toIndex(value, index):\n",
    "    if (value > 0.5):\n",
    "        return index\n",
    "    else:\n",
    "        return    \n",
    "\n",
    "myDf['features'] = myDf['features'].map(lambda x : filter(None, x) if type(x) is list else [x] )\n",
    "\n",
    "message_seperator = vocabSize\n",
    "num_features = vocabSize + 1\n",
    "\n",
    "myDf = myDf.drop(columns=['nlp_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "byComId = myDf.groupby('comId', as_index=False).agg({'status':'mean',\n",
    "                                        'features': lambda series: reduce(lambda x, y: np.append(np.append(np.array(x).astype(int), message_seperator), np.array(y).astype(int)), series), \n",
    "                                        'msgId' : 'count' })  \n",
    "\n",
    "byComId = byComId[byComId['msgId'] > 2]\n",
    "byComId = shuffle(byComId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sklearn.metrics as sklm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import Callback,TensorBoard,EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Embedding, LSTM, Dropout, MaxPooling1D, Activation, concatenate\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPooling1D,Bidirectional, Reshape\n",
    "from keras import regularizers\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# maximum lenght of word per thread\n",
    "seq_length = 500\n",
    "epochs = 25\n",
    "batch_size=2048\n",
    "class_weight = {0:2,1:8}\n",
    "num_features_all = num_features\n",
    "features = byComId['features'].map(lambda x: np.array(x)[-seq_length:]).values\n",
    "\n",
    "print type(features[1])\n",
    "\n",
    "X =  pad_sequences(features, maxlen=seq_length)\n",
    "y = byComId['status'].astype(int)\n",
    "\n",
    "print X.shape\n",
    "\n",
    "print np.mean(y)\n",
    "print np.min(y)\n",
    "print np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Embedding, LSTM, Dropout, MaxPooling1D, Activation\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPooling1D,Bidirectional\n",
    "from keras import regularizers\n",
    "import types, copy\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    \"\"\"Matthews correlation metric.\n",
    "    It is only computed as a batch-wise average, not globally.\n",
    "    Computes the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "\n",
    "class KerasBatchClassifier(KerasClassifier):\n",
    "    def fit(self, X, y, **kwargs):        \n",
    "        if self.build_fn is None:\n",
    "            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n",
    "        elif not isinstance(self.build_fn, types.FunctionType) and not isinstance(self.build_fn, types.MethodType):\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn.__call__))\n",
    "        else:\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor=\"val_matthews_correlation\", patience=3, verbose=1, mode=\"max\")\n",
    "        model_checkpoint = ModelCheckpoint(\"results/best_weights.{epoch:02d}-{matthews_correlation:.5f}.hdf5\", monitor=\"matthews_correlation\", \n",
    "                                           verbose=1, save_best_only=True, mode=\"max\", period=3)\n",
    "#         tensorboard = TensorBoard(log_dir='./graph', histogram_freq=0,  \n",
    "#           write_graph=True, write_images=True)\n",
    "        callbacks = [early_stopping]\n",
    "    \n",
    "        x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)   \n",
    "        \n",
    "        self.__history = self.model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=[x_test, y_test],\n",
    "            class_weight=class_weight,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "                   )\n",
    "   \n",
    "        return self.__history\n",
    "\n",
    "\n",
    "    def score(self, X, y, **kwargs):\n",
    "        loss_name = self.model.loss\n",
    "        if hasattr(loss_name, '__name__'):\n",
    "            loss_name = loss_name.__name__\n",
    "        outputs = self.model.evaluate(X, y)\n",
    "        if type(outputs) is not list:\n",
    "            outputs = [outputs]\n",
    "        for name, output in zip(self.model.metrics_names, outputs):\n",
    "            if name == 'matthews_correlation':\n",
    "                print ('MCC: ' + str(output))\n",
    "                return output\n",
    "\n",
    "        raise Exception('The model is not configured to compute mcc. '\n",
    "                        'You should pass `metrics=[\"accuracy\"]` to '\n",
    "                        'the `model.compile()` method.')\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        return self.__history   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def create_model(embedding_size ,recurrent_reg, kernel_reg, lstm_size, dropout):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_features, output_dim=embedding_size, input_length=seq_length))\n",
    "    model.add(LSTM(lstm_size, \n",
    "                kernel_regularizer=regularizers.l2(recurrent_reg), \n",
    "                dropout=dropout, recurrent_dropout=dropout, return_sequences=True, input_shape=(seq_length,num_features)))\n",
    "    model.add(LSTM(lstm_size, \n",
    "                kernel_regularizer=regularizers.l2(recurrent_reg), \n",
    "                dropout=dropout, recurrent_dropout=dropout, return_sequences=True))\n",
    "    model.add(LSTM(lstm_size, \n",
    "                kernel_regularizer=regularizers.l2(recurrent_reg), \n",
    "                dropout=dropout, recurrent_dropout=dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=[matthews_correlation, 'accuracy'])    \n",
    "    return model\n",
    "\n",
    "parameters = {'recurrent_reg': [0.1,0.2],\n",
    "              'lstm_size': [32,48,64],\n",
    "              'dropout': [0.3,0.4],\n",
    "              'kernel_reg': [0.2,0.3],\n",
    "              'embedding_size': [32,48,64]            \n",
    "              }\n",
    "\n",
    "\n",
    "model = KerasBatchClassifier(build_fn=create_model, epochs=epochs, shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameters,return_train_score=False)\n",
    "grid_result = grid.fit(X,y)\n",
    "grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_result.best_score_)\n",
    "print(grid_result.best_params_)\n",
    "print(grid_result.cv_results_['mean_test_score'])\n",
    "print(grid_result.cv_results_['split0_test_score'].mean() )\n",
    "print(grid_result.cv_results_['split1_test_score'].mean() )\n",
    "print(grid_result.cv_results_['split2_test_score'].mean() )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python2)",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
